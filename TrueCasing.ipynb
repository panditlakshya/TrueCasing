{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrueCasing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPSXlf2fRkavg4txhWCkMsw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panditlakshya/TrueCasing/blob/main/TrueCasing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veyJotNpaIli",
        "outputId": "1a1c4355-895b-45b6-8ece-a5dcec4ce96f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI93x4MkaXzs"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import re\n",
        "def truecasing_by_sentence_segmentation(input_text):\n",
        "    # split the text into sentences\n",
        "    sentences = sent_tokenize(input_text, language='english')\n",
        "    print(sentences)\n",
        "    # capitalize the sentences\n",
        "    sentences_capitalized = [s.capitalize() for s in sentences]\n",
        "    # join the capitalized sentences\n",
        "    text_truecase = re.sub(\" (?=[\\.,'!?:;])\", \"\", ' '.join(sentences_capitalized))\n",
        "    return text_truecase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlxtor5FbPVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf306af-473c-4975-fd38-6aaa591c8dcf"
      },
      "source": [
        "# text = \"I think that john stone is a nice guy. there is a stone on the grass. i'm fat. are you welcome and smart in london? is this martin's dog?\"\n",
        "tempTruecased=truecasing_by_sentence_segmentation(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['stanford corenlp also provides a set of powerful tools.', 'it can detect the base forms of words (lemma), parts of speech, names of companies, people, etc.', 'it can also normalize dates, times and numeric quantities.', 'it is also used to mark up phrases and syntactic dependencies, to indicate sentiment, and to get the quotes people said.', 'stanfordnlp takes few lines of code to start utilizing corenlp’s sophisticated api.', 'for those who want to get deeper, check the post linked here.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np_0q76zL33S",
        "outputId": "af6e90f9-6592-4c48-d7ba-defbb801a1f7"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=4b2d7f7bfafecba92e8bd38368cef8d8382124b60373d309abf897750b860ab6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mu7tju2a/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo7flePJLMIs"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "# spacy_nlp = spacy.load('en_core_web_lg')\n",
        "text=\"stanford corenlp also provides a set of powerful tools. it can detect the base forms of words (lemma), parts of speech, names of companies, people, etc. it can also normalize dates, times and numeric quantities. it is also used to mark up phrases and syntactic dependencies, to indicate sentiment, and to get the quotes people said. stanfordnlp takes few lines of code to start utilizing corenlp’s sophisticated api. for those who want to get deeper, check the post linked here.\"\n",
        "doc = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJs6NtROSeSX",
        "outputId": "0add6659-723c-4a89-d5cd-5ac9b92e9ce4"
      },
      "source": [
        "[(w.text, w.tag_) for w in doc]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('stanford', 'NNP'),\n",
              " ('corenlp', 'NNP'),\n",
              " ('also', 'RB'),\n",
              " ('provides', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('set', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('powerful', 'JJ'),\n",
              " ('tools', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('it', 'PRP'),\n",
              " ('can', 'MD'),\n",
              " ('detect', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('base', 'NN'),\n",
              " ('forms', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('words', 'NNS'),\n",
              " ('(', '-LRB-'),\n",
              " ('lemma', 'NN'),\n",
              " (')', '-RRB-'),\n",
              " (',', ','),\n",
              " ('parts', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('speech', 'NN'),\n",
              " (',', ','),\n",
              " ('names', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('companies', 'NNS'),\n",
              " (',', ','),\n",
              " ('people', 'NNS'),\n",
              " (',', ','),\n",
              " ('etc', 'FW'),\n",
              " ('.', '.'),\n",
              " ('it', 'PRP'),\n",
              " ('can', 'MD'),\n",
              " ('also', 'RB'),\n",
              " ('normalize', 'VB'),\n",
              " ('dates', 'NNS'),\n",
              " (',', ','),\n",
              " ('times', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('numeric', 'JJ'),\n",
              " ('quantities', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('it', 'PRP'),\n",
              " ('is', 'VBZ'),\n",
              " ('also', 'RB'),\n",
              " ('used', 'VBN'),\n",
              " ('to', 'TO'),\n",
              " ('mark', 'VB'),\n",
              " ('up', 'RP'),\n",
              " ('phrases', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('syntactic', 'JJ'),\n",
              " ('dependencies', 'NNS'),\n",
              " (',', ','),\n",
              " ('to', 'TO'),\n",
              " ('indicate', 'VB'),\n",
              " ('sentiment', 'NN'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('to', 'TO'),\n",
              " ('get', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('quotes', 'NNS'),\n",
              " ('people', 'NNS'),\n",
              " ('said', 'VBD'),\n",
              " ('.', '.'),\n",
              " ('stanfordnlp', 'NNP'),\n",
              " ('takes', 'VBZ'),\n",
              " ('few', 'JJ'),\n",
              " ('lines', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('code', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('start', 'VB'),\n",
              " ('utilizing', 'VBG'),\n",
              " ('corenlp', 'NNP'),\n",
              " ('’s', 'POS'),\n",
              " ('sophisticated', 'JJ'),\n",
              " ('api', 'NN'),\n",
              " ('.', '.'),\n",
              " ('for', 'IN'),\n",
              " ('those', 'DT'),\n",
              " ('who', 'WP'),\n",
              " ('want', 'VBP'),\n",
              " ('to', 'TO'),\n",
              " ('get', 'VB'),\n",
              " ('deeper', 'JJR'),\n",
              " (',', ','),\n",
              " ('check', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('post', 'NN'),\n",
              " ('linked', 'VBN'),\n",
              " ('here', 'RB'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQCIZ7ciTQ1G",
        "outputId": "40e60152-a925-4344-cfda-2bbeed122c9f"
      },
      "source": [
        "print([(w.text, w.label_) for w in words.ents])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('john stone', 'PERSON'), ('london', 'GPE'), ('martin', 'PERSON')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVa3bC5oThkT",
        "outputId": "eed35c99-3056-43d4-f3df-b286146997e8"
      },
      "source": [
        "print([(w, w.ent_iob_, w.ent_type_) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(I, 'O', ''), (think, 'O', ''), (that, 'O', ''), (john, 'B', 'PERSON'), (stone, 'I', 'PERSON'), (is, 'O', ''), (a, 'O', ''), (nice, 'O', ''), (guy, 'O', ''), (., 'O', ''), (there, 'O', ''), (is, 'O', ''), (a, 'O', ''), (stone, 'O', ''), (on, 'O', ''), (the, 'O', ''), (grass, 'O', ''), (., 'O', ''), (i, 'O', ''), ('m, 'O', ''), (fat, 'O', ''), (., 'O', ''), (are, 'O', ''), (you, 'O', ''), (welcome, 'O', ''), (and, 'O', ''), (smart, 'O', ''), (in, 'O', ''), (london, 'B', 'GPE'), (?, 'O', ''), (is, 'O', ''), (this, 'O', ''), (martin, 'B', 'PERSON'), ('s, 'O', ''), (dog, 'O', ''), (?, 'O', '')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax2bYE5NyuH_",
        "outputId": "304a253d-d017-4c5f-d013-e4181862c50b"
      },
      "source": [
        "import re\n",
        "\n",
        "tagged_sent = [(w.text, w.tag_) for w in doc]\n",
        "normalized_sent = [w.capitalize() if t in [\"NNPS\",\"NNP\"] else w for (w,t) in tagged_sent]\n",
        "normalized_sent[0] = normalized_sent[0].capitalize()\n",
        "string = re.sub(\" (?=[\\.,'!?:;])\", \"\", ' '.join(normalized_sent))\n",
        "print(string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stanford Corenlp also provides a set of powerful tools. it can detect the base forms of words ( lemma ), parts of speech, names of companies, people, etc. it can also normalize dates, times and numeric quantities. it is also used to mark up phrases and syntactic dependencies, to indicate sentiment, and to get the quotes people said. Stanfordnlp takes few lines of code to start utilizing Corenlp ’s sophisticated api. for those who want to get deeper, check the post linked here.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRKkX8bXcCrl"
      },
      "source": [
        "**Start**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4klU4r6ob5jy"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMae3vEqcGA1",
        "outputId": "0d001cda-6351-4948-cefc-191204f95929"
      },
      "source": [
        "import nltk\n",
        "import spacy\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import re\n",
        "def truecasing_by_sentence_segmentation(input_text):\n",
        "    # split the text into sentences\n",
        "    sentences = sent_tokenize(input_text, language='english')\n",
        "    print(sentences)\n",
        "    # capitalize the sentences\n",
        "    sentences_capitalized = [s.capitalize() for s in sentences]\n",
        "    # join the capitalized sentences\n",
        "    text_truecase = re.sub(\" (?=[\\.,'!?:;])\", \"\", ' '.join(sentences_capitalized))\n",
        "    return text_truecase\n",
        "\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "text = \"I think that john stone is a nice guy. there is a stone on the grass. i'm fat. are you welcome and smart in london? is this martin's dog?\"\n",
        "doc = nlp(truecasing_by_sentence_segmentation(text))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['I think that john stone is a nice guy.', 'there is a stone on the grass.', \"i'm fat.\", 'are you welcome and smart in london?', \"is this martin's dog?\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "979ZFswQiCAI"
      },
      "source": [
        "**finish**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4-E3ml6SRnh"
      },
      "source": [
        "import nltk, re\n",
        "\n",
        "def truecase(text):\n",
        "    truecased_sents = [] # list of truecased sentences\n",
        "    # apply POS-tagging\n",
        "    tagged_sent = nltk.pos_tag([word.lower() for word in nltk.word_tokenize(text)])\n",
        "    print(tagged_sent)\n",
        "    # infer capitalization from POS-tags\n",
        "    normalized_sent = [w.capitalize() if t in [\"NNP\",\"NNPS\"] else w for (w,t) in tagged_sent]\n",
        "    # capitalize first word in sentence\n",
        "    normalized_sent[0] = normalized_sent[0].capitalize()\n",
        "    # use regular expression to get punctuation right\n",
        "    pretty_string = re.sub(\" (?=[\\.,'!?:;])\", \"\", ' '.join(normalized_sent))\n",
        "    return pretty_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "viA1NYX8SWLN",
        "outputId": "3fb74c9c-0ecd-41fb-cc0d-9ae07b83501f"
      },
      "source": [
        "# nltk.download('averaged_perceptron_tagger')\n",
        "text = \"Clonazepam Has Been Approved As An Anticonvulsant To Be Manufactured In 0.5mg, 1mg And 2mg Tablets. It Is The Generic Equivalent Of Roche Laboratories' Klonopin.\"\n",
        "truecase(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('clonazepam', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('approved', 'VBN'), ('as', 'IN'), ('an', 'DT'), ('anticonvulsant', 'NN'), ('to', 'TO'), ('be', 'VB'), ('manufactured', 'VBN'), ('in', 'IN'), ('0.5mg', 'CD'), (',', ','), ('1mg', 'CD'), ('and', 'CC'), ('2mg', 'CD'), ('tablets', 'NNS'), ('.', '.'), ('it', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ('generic', 'JJ'), ('equivalent', 'NN'), ('of', 'IN'), ('roche', 'NN'), ('laboratories', 'NNS'), (\"'\", 'POS'), ('klonopin', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Clonazepam has been approved as an anticonvulsant to be manufactured in 0.5mg, 1mg and 2mg tablets. it is the generic equivalent of roche laboratories' klonopin.\""
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXIkDOEZXB39"
      },
      "source": [
        "import string\n",
        "import math\n",
        "import pickle\n",
        "\"\"\"\n",
        "This file contains the functions to truecase a sentence.\n",
        "\"\"\"\n",
        "\n",
        "def getScore(prevToken, possibleToken, nextToken, wordCasingLookup, uniDist, backwardBiDist, forwardBiDist, trigramDist):\n",
        "    pseudoCount = 5.0\n",
        "    \n",
        "    #Get Unigram Score\n",
        "    nominator = uniDist[possibleToken]+pseudoCount    \n",
        "    denominator = 0    \n",
        "    for alternativeToken in wordCasingLookup[possibleToken.lower()]:\n",
        "        denominator += uniDist[alternativeToken]+pseudoCount\n",
        "        \n",
        "    unigramScore = nominator / denominator\n",
        "        \n",
        "        \n",
        "    #Get Backward Score  \n",
        "    bigramBackwardScore = 1\n",
        "    if prevToken != None:  \n",
        "        nominator = backwardBiDist[prevToken+'_'+possibleToken]+pseudoCount\n",
        "        denominator = 0    \n",
        "        for alternativeToken in wordCasingLookup[possibleToken.lower()]:\n",
        "            denominator += backwardBiDist[prevToken+'_'+alternativeToken]+pseudoCount\n",
        "            \n",
        "        bigramBackwardScore = nominator / denominator\n",
        "        \n",
        "    #Get Forward Score  \n",
        "    bigramForwardScore = 1\n",
        "    if nextToken != None:  \n",
        "        nextToken = nextToken.lower() #Ensure it is lower case\n",
        "        nominator = forwardBiDist[possibleToken+\"_\"+nextToken]+pseudoCount\n",
        "        denominator = 0    \n",
        "        for alternativeToken in wordCasingLookup[possibleToken.lower()]:\n",
        "            denominator += forwardBiDist[alternativeToken+\"_\"+nextToken]+pseudoCount\n",
        "            \n",
        "        bigramForwardScore = nominator / denominator\n",
        "        \n",
        "        \n",
        "    #Get Trigram Score  \n",
        "    trigramScore = 1\n",
        "    if prevToken != None and nextToken != None:  \n",
        "        nextToken = nextToken.lower() #Ensure it is lower case\n",
        "        nominator = trigramDist[prevToken+\"_\"+possibleToken+\"_\"+nextToken]+pseudoCount\n",
        "        denominator = 0    \n",
        "        for alternativeToken in wordCasingLookup[possibleToken.lower()]:\n",
        "            denominator += trigramDist[prevToken+\"_\"+alternativeToken+\"_\"+nextToken]+pseudoCount\n",
        "            \n",
        "        trigramScore = nominator / denominator\n",
        "        \n",
        "    result = math.log(unigramScore) + math.log(bigramBackwardScore) + math.log(bigramForwardScore) + math.log(trigramScore)\n",
        "  \n",
        "  \n",
        "    return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtmomPNqiFkj"
      },
      "source": [
        "def getTrueCase(tokens, outOfVocabularyTokenOption, wordCasingLookup, uniDist, backwardBiDist, forwardBiDist, trigramDist):\n",
        "    \"\"\"\n",
        "    Returns the true case for the passed tokens.\n",
        "    @param tokens: Tokens in a single sentence\n",
        "    @param outOfVocabulariyTokenOption:\n",
        "        title: Returns out of vocabulary (OOV) tokens in 'title' format\n",
        "        lower: Returns OOV tokens in lower case\n",
        "        as-is: Returns OOV tokens as is\n",
        "    \"\"\"\n",
        "    tokensTrueCase = []\n",
        "    for tokenIdx in range(len(tokens)):\n",
        "        token = tokens[tokenIdx]\n",
        "        if token in string.punctuation or token.isdigit():\n",
        "            tokensTrueCase.append(token)\n",
        "        else:\n",
        "            if token in wordCasingLookup:\n",
        "                if len(wordCasingLookup[token]) == 1:\n",
        "                    tokensTrueCase.append(list(wordCasingLookup[token])[0])\n",
        "                else:\n",
        "                    prevToken = tokensTrueCase[tokenIdx-1] if tokenIdx > 0  else None\n",
        "                    nextToken = tokens[tokenIdx+1] if tokenIdx < len(tokens)-1 else None\n",
        "                    \n",
        "                    bestToken = None\n",
        "                    highestScore = float(\"-inf\")\n",
        "                    \n",
        "                    for possibleToken in wordCasingLookup[token]:\n",
        "                        score = getScore(prevToken, possibleToken, nextToken, wordCasingLookup, uniDist, backwardBiDist, forwardBiDist, trigramDist)\n",
        "                           \n",
        "                        if score > highestScore:\n",
        "                            bestToken = possibleToken\n",
        "                            highestScore = score\n",
        "                        \n",
        "                    tokensTrueCase.append(bestToken)\n",
        "                    \n",
        "                if tokenIdx == 0:\n",
        "                    tokensTrueCase[0] = tokensTrueCase[0].title();\n",
        "                    \n",
        "            else: #Token out of vocabulary\n",
        "                if outOfVocabularyTokenOption == 'title':\n",
        "                    tokensTrueCase.append(token.title())\n",
        "                elif outOfVocabularyTokenOption == 'lower':\n",
        "                    tokensTrueCase.append(token.lower())\n",
        "                else:\n",
        "                    tokensTrueCase.append(token) \n",
        "    \n",
        "    return tokensTrueCase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmkPVOFCmRyP",
        "outputId": "adad4792-261e-4fbb-94c0-d3f873f8382a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdWMU129nBMm",
        "outputId": "e2c348c1-a85f-4bea-d071-781a248ad28a"
      },
      "source": [
        "cd drive/MyDrive/distributions.obj\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distributions.obj  \u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEUkCYTTXAg2"
      },
      "source": [
        "f = open('drive/MyDrive/distributions.obj', 'rb')\n",
        "uniDist = pickle.load(f)\n",
        "backwardBiDist = pickle.load(f)\n",
        "forwardBiDist = pickle.load(f)\n",
        "trigramDist = pickle.load(f)\n",
        "wordCasingLookup = pickle.load(f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Sno3x6EIiLBc",
        "outputId": "c3648a05-049c-4e3e-d869-0d501e7fe3f9"
      },
      "source": [
        "def truecasing_by_stats(input_text):\n",
        "    truecase_text = ''\n",
        "    sentences = sent_tokenize(input_text, language='english')\n",
        "    for s in sentences:\n",
        "        tokens = [token.lower() for token in nltk.word_tokenize(s)]\n",
        "        tokensTrueCase = getTrueCase(tokens, 'lower', wordCasingLookup, uniDist, backwardBiDist, forwardBiDist, trigramDist)\n",
        "        sTrueCase = re.sub(\" (?=[\\.,'!?:;])\", \"\", ' '.join(tokensTrueCase))\n",
        "        truecase_text = truecase_text + sTrueCase + ' '\n",
        "    return truecase_text.strip()\n",
        "\n",
        "# text = \"stanford corenlp also provides a set of powerful tools. it can detect the base forms of words (lemma), parts of speech, names of companies, people, etc. it can also normalize dates, times and numeric quantities. it is also used to mark up phrases and syntactic dependencies, to indicate sentiment, and to get the quotes people said. stanfordnlp takes few lines of code to start utilizing corenlp’s sophisticated api. for those who want to get deeper, check the post linked here.\"\n",
        "text=\"write programs that perform true casing on a given english sentence utilizing dynamic programming technique. in addition to your report, further submit your program and your program output to canvas.\"\n",
        "truecasing_by_stats(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Write programs that perform true casing on a given English sentence utilizing dynamic programming technique. In addition to your report, further submit your program and your program output to canvas.'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}